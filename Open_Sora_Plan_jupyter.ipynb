{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/Open-Sora-Plan-jupyter/blob/main/Open_Sora_Plan_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b dev https://github.com/camenduru/Open-Sora-Plan-v1.0.0-hf\n",
        "%cd /content/Open-Sora-Plan-v1.0.0-hf\n",
        "!pip install -q diffusers==0.24.0 gradio==3.50.2 einops==0.7.0 omegaconf==2.1.1 pytorch-lightning==1.4.2 torchmetrics==0.6.0 torchtext==0.6 accelerate==0.28.0\n",
        "\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "import torch\n",
        "from diffusers import PNDMScheduler\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from gradio.components import Textbox, Video, Image\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "\n",
        "from opensora.models.ae import ae_stride_config, getae, getae_wrapper\n",
        "from opensora.models.diffusion.latte.modeling_latte import LatteT2V\n",
        "from opensora.sample.pipeline_videogen import VideoGenPipeline\n",
        "from opensora.serve.gradio_utils import block_css, title_markdown, randomize_seed_fn, set_env, examples, DESCRIPTION\n",
        "\n",
        "def generate_img(prompt, sample_steps, scale, seed=0, randomize_seed=False, force_images=False):\n",
        "    seed = int(randomize_seed_fn(seed, randomize_seed))\n",
        "    set_env(seed)\n",
        "    video_length = transformer_model.config.video_length if not force_images else 1\n",
        "    height, width = int(args.version.split('x')[1]), int(args.version.split('x')[2])\n",
        "    num_frames = 1 if video_length == 1 else int(args.version.split('x')[0])\n",
        "    with torch.no_grad():\n",
        "        videos = videogen_pipeline(prompt,\n",
        "                                   video_length=video_length,\n",
        "                                   height=height,\n",
        "                                   width=width,\n",
        "                                   num_inference_steps=sample_steps,\n",
        "                                   guidance_scale=scale,\n",
        "                                   enable_temporal_attentions=not force_images,\n",
        "                                   num_images_per_prompt=1,\n",
        "                                   mask_feature=True,\n",
        "                                   ).video\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    videos = videos[0]\n",
        "    tmp_save_path = 'tmp.mp4'\n",
        "    imageio.mimwrite(tmp_save_path, videos, fps=24, quality=9)  # highest quality is 10, lowest is 0\n",
        "    display_model_info = f\"Video size: {num_frames}×{height}×{width}, \\nSampling Step: {sample_steps}, \\nGuidance Scale: {scale}\"\n",
        "    return tmp_save_path, prompt, display_model_info, seed\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = type('args', (), {\n",
        "        'ae': 'CausalVAEModel_4x8x8',\n",
        "        'force_images': False,\n",
        "        'model_path': 'LanguageBind/Open-Sora-Plan-v1.0.0',\n",
        "        'text_encoder_name': 'DeepFloyd/t5-v1_1-xxl',\n",
        "        'version': '65x512x512'\n",
        "    })\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Load model:\n",
        "    transformer_model = LatteT2V.from_pretrained(args.model_path, subfolder=args.version, torch_dtype=torch.float16, cache_dir='cache_dir').to(device)\n",
        "\n",
        "    vae = getae_wrapper(args.ae)(args.model_path, subfolder=\"vae\", cache_dir='cache_dir').to(device, dtype=torch.float16)\n",
        "    vae.vae.enable_tiling()\n",
        "    image_size = int(args.version.split('x')[1])\n",
        "    latent_size = (image_size // ae_stride_config[args.ae][1], image_size // ae_stride_config[args.ae][2])\n",
        "    vae.latent_size = latent_size\n",
        "    transformer_model.force_images = args.force_images\n",
        "    tokenizer = T5Tokenizer.from_pretrained(args.text_encoder_name, cache_dir=\"cache_dir\")\n",
        "    text_encoder = T5EncoderModel.from_pretrained(args.text_encoder_name, cache_dir=\"cache_dir\",\n",
        "                                                  torch_dtype=torch.float16).to(device)\n",
        "\n",
        "    # set eval mode\n",
        "    transformer_model.eval()\n",
        "    vae.eval()\n",
        "    text_encoder.eval()\n",
        "    scheduler = PNDMScheduler()\n",
        "    videogen_pipeline = VideoGenPipeline(vae=vae,\n",
        "                                         text_encoder=text_encoder,\n",
        "                                         tokenizer=tokenizer,\n",
        "                                         scheduler=scheduler,\n",
        "                                         transformer=transformer_model).to(device=device)\n",
        "\n",
        "\n",
        "    demo = gr.Interface(\n",
        "        fn=generate_img,\n",
        "        inputs=[Textbox(label=\"\",\n",
        "                        placeholder=\"Please enter your prompt. \\n\"),\n",
        "                gr.Slider(\n",
        "                    label='Sample Steps',\n",
        "                    minimum=1,\n",
        "                    maximum=500,\n",
        "                    value=50,\n",
        "                    step=10\n",
        "                ),\n",
        "                gr.Slider(\n",
        "                    label='Guidance Scale',\n",
        "                    minimum=0.1,\n",
        "                    maximum=30.0,\n",
        "                    value=10.0,\n",
        "                    step=0.1\n",
        "                ),\n",
        "                gr.Slider(\n",
        "                    label=\"Seed\",\n",
        "                    minimum=0,\n",
        "                    maximum=203279,\n",
        "                    step=1,\n",
        "                    value=0,\n",
        "                ),\n",
        "                gr.Checkbox(label=\"Randomize seed\", value=True),\n",
        "                gr.Checkbox(label=\"Generate image (1 frame video)\", value=False),\n",
        "                ],\n",
        "        outputs=[Video(label=\"Vid\", width=512, height=512),\n",
        "                 Textbox(label=\"input prompt\"),\n",
        "                 Textbox(label=\"model info\"),\n",
        "                 gr.Slider(label='seed')],\n",
        "        title=title_markdown, description=DESCRIPTION, theme=gr.themes.Default(), css=block_css,\n",
        "        examples=examples,\n",
        "    )\n",
        "    demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
